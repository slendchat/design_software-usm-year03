# Отчет по лабе 05

Цепочкуа чтения\обработки\записи

## 1. Общая картина
Суть такая- Есть пайплайн `student_pipeline`, который тащит студенческие записи из разных источников (файл, http, рандом), гоняет их через  трансформации и потом выдает в другом формате (json/csv/текст). Все эти штуки разнесены по разным модулям, шоб не нарушать SRP иначе сложно анализировать. У меня есть:
- `domain.py` - тут лежат все dto и так называемые tagged union'ы.
- `readers.py` - ридеры, каждый отвечает за свой источник.
- `transforms.py` - чистые функции + адаптеры, чтоб в DI запихнуть.
- `writers.py` - разные способы сохранить файлы.
- `pipeline.py` - дирижер, который всех зовет.
- `container.py` - примитивный DI контейнер, но рабочий.
- `main.py` - cli морда, которая собирает конфиг из аргументов.

## 2. Domain и конфиги
В `domain.py` я делал основу. Там `StudentRecord` с полями `student_id`, `full_name`, `group`, `average_score`, `credits`. Еще есть структуры:
- `ScoreFilterParams` + `SortParams` и `TransformationConfig` - чтоб аккуратно упаковывать все параметры фильтров/сортировок. Так намного удобней, чем гонять длинные списки аргументов.
- `FileSourceDefinition`, `HttpSourceDefinition`, `RandomSourceDefinition` - тут я храним откуда именно подтягивать данные.
- `JsonFormatContext`, `CsvFormatContext`, `PlainTextFormatContext` - описывают куда пилить вывод. Это и есть tagged union, просто в виде юниона типов.
- `PipelineConfig` собирает все сразу: источник, назначение, трансформации.

Важно помнить, что все эти штуки независимые. Если надо добавить новый формат или источник, я просто дописываю новый класс в домейне, не трогая остальное. Ну типо почти не трогая, потому что что reader/writer все равно надо реализовать.

## 3. Readers
`readers.py` содержит интерфейс `SourceReader` (формально Protocol)). Дальше есть три реализации:
1. `FileSourceReader` - открывает json файл, грузит список и конвертит в `StudentRecord`. Тут есть helper `_dict_to_record`, который и вытаскивает нужные поля даже если ключи отличаются (например вместо `student_id` ктото записал просто `id`).
2. `HttpSourceReader` - почти то же самое, но через `urllib.request`. Синхронный запрос, возвращает json.
3. `RandomSourceReader` - генерит фейковые записи, если вдруг надо просто прогнать весь пайплайн без реальных данных. Генерация имен через случайные буквы.

`SourceReaderResolver` - это как небольшой сервис куда я засовываю все доступные ридеры и потом прошу дать мне нужный под этот SourceDefinition. По сути это инверсия зависимостей

## 4. Transforms
В `transforms.py` я сделал две чистые функции:
- `filter_by_score` - пробегает по записям и убирает тех, у кого балл вне диапазона или группа попала в игнор. Я специально все проверки сделал в одном месте, чтобы не путать порядок фильтров.
- `sort_and_limit` - сортирует по `average_score`, `full_name` или `credits` и при необходимости обрезает список до `limit`. Тут я проверяю ключ заранее и бросаю ошибку, если пользователь чтото странное указал.

И самое главное - адаптеры `FilterTransform` и `SortTransform`, которые реализуют `RecordTransform.apply`. Это позволяет DI контейнеру оперировать объектами, а внутри уже вызывается чистая функция.

## 5. Writers
`writers.py` симметричен читателям:
1. `JsonDestinationWriter` - тупо `json.dump`.
2. `CsvDestinationWriter` - использует `csv.DictWriter`, пишет шапку и каждую запись.
3. `PlainTextDestinationWriter` - делает человеческий лог по типу Имя (ID) - группа - баллы, кредиты.

Анологичный `DestinationWriterResolver`, который по DestinationFormat выбирает конкретный writer. Все зеркально инпуту, поэтому легко расширять.

## 6. Pipeline
В `pipeline.py` два класса. `TransformationBuilder` читает `TransformationConfig` и создает список трансформаций. Если конфиг пустой - трансформаций нет, все ок. `StudentPipelineService` делает главную работу:
1. `reader_resolver.resolve` вытаскивает нужный ридер.
2. `reader.read` получает список записей.
3. `transformation_builder.build` дает список шагов, и я их по очереди применяю (`records = transform.apply(records)`).
4. `writer_resolver.resolve` находит куда писать, `writer.write` уже пишет.

Метод `execute` еще возвращает список строк с логами, чтобы CLI что-то отображал. Так легче отлаживать: я просто вижу сколько записей и какой writer использовался.

## 7. Контейнер
`container.py` - очень простой DI. Я регистрирую три ридера, три райтера, резолверы и сам пайплайн. В `ServiceContainer` есть `register_singleton` и `resolve`. Главное, что `pipeline_service` потом резолвит остальные зависимости через контейнер. Можно было бы юзать что-то типа `dependency_injector`, но не успел.

## 8. CLI вход
`main.py` - это точка входа: парсит аргументы через argparse. Есть флаги:
- `--source` (`file`, `http`, `random`)
- `--input` (путь или url, если надо)
- `--destination` (`json`, `csv`, `text`)
- `--output`
- плюс набор флагов для фильтров min/max score, ignored group, sort key, ascending, limit, random-count, http-timeout

Функции `build_source`, `build_destination`, `build_transform_config` переводят флаги в доменные структуры. Потом я создаю `PipelineConfig`, строю контейнер, беру `pipeline_service` и запускаю `execute`. В конце просто печатаю лог по строкам. Проверял командой:
```
python3 -m student_pipeline.main \
  --source file \
  --input sample_students.json \
  --destination text \
  --output sample_output.txt
```

## 9. Пример данных
В `src/sample_students.json` есть 4 записи.

## 10. Как работает
Алгоритм прост:
1. Собираем конфиг из консоли (все в `main.py`).
2. Контейнер из `container.py` создает ридеры/врайтеры и резолверы.
3. `StudentPipelineService` выполняет пайплайн (читает, трансформирует, пишет).
4. CLI печатает инфу и отдает `0`.


## 11. Нюансы
- `RandomSourceReader` создает имена из случайных букв, но чтобы не выглядело как мусор, я капитализировал первую букву.
- `filter_by_score` приводит названия групп к нижнему регистру, иначе кто-то введет cs-01 и фильтр не поймет.
- `sort_and_limit` валится если указать ключ вне списка. Лучше пусть будут ошибки, чем сортировать не пойми как.
- JSON writer пишет pretty с отступами, CSV writer фиксирует порядок колонок.
- Plain text writer просто удобен для глаз, когда надо глянуть что происходит.
- Контейнер возвращает одно и то же (singletons), значит стоит аккуратно использовать, но у нас нет mutable state в сервиса, так что норм.
